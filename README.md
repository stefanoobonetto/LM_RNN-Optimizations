# Enhanced RNN: Techniques and Hyperparameter Optimization for Improved Performance

This project explores enhancements to the RNN architecture to improve performance in NLP tasks, focusing on reducing perplexity. 

In Part 1, modifications including replacing the RNN with an LSTM network, adding dropout layers, and switching to the AdamW optimizer are implemented incrementally. For each experiment, perplexity is evaluated, with the goal of achieving values below 250. 

In Part 2, additional techniques such as weight tying, variational dropout, and non-monotonically triggered AvSGD are applied to further optimize the model's performance while ensuring perplexity remains below 250 and lower than the base LSTM model achieved in Part 1.

## Contacts

Stefano Bonetto: [contact me on LinkedIn](https://www.linkedin.com/in/stefano-bonetto)
