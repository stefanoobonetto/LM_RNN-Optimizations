# Multi-Task Learning with BERT for Intent Classification and Slot Filling

This repository contains code implementations for multi-task learning using BERT-based models for intent classification and slot filling tasks. 

The code includes modifications to baseline architectures, such as adding bidirectionality and dropout layers, as well as fine-tuning pre-trained BERT models on the ATIS dataset. 

The aim is to achieve high accuracy for intent classification and F1 score with conll for slot filling, while addressing challenges such as sub-tokenization. The repository provides a comprehensive solution for natural language understanding tasks in the context of conversational AI systems.
